{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "poa0_c7TVSi9",
        "uifTn8ER7vAu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanmad/machine-learning-exercises/blob/main/GradientDescent_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MavRPT-dfP1"
      },
      "source": [
        "**Copyright: Â© NexStream Technical Institute, LLC**.  \n",
        "All rights reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poa0_c7TVSi9"
      },
      "source": [
        "#**PART 1**:  Gradient Descent with Simple Linear Regression\n",
        "In this project, you will generate a Simple Linear Regression model using gradient descent to minimize the cost function of the sum of squared errors.  You will compare the coefficients computed directly for the model presented in the Linear Regression Project in a previous unit, then you will recalculate the coefficients using the gradient descent cost minimization technique.\n",
        "\n",
        "Please reference the video lectures on Gradient Descent, and Simple Linear Regression for a description of this example and the main functions you will need in the implementation.\n",
        "Please complete the following steps in your Colab Script.  The reference script below provides template code and hints to help with each step.  You will be turning in code and screenshots of your console output in an accompanying assignment.\n",
        "\n",
        "\n",
        "-  **Step 1**:  \n",
        "Calculate the coefficients for a simple linear regression model using the following equations:  (NOTE, you may NOT use any machine learning library models for this step - you must calculate the parameters use the equations shown).  \n",
        "Normalize the dataset using the min and max of each of the variables (min/max of the X's and min/max of the y's), then use the equation shown to normalize each of the values in the columns:\n",
        "          X_normalized = (X - Xmin)/(Xmax - Xmin)\n",
        "          y_normalized = (y - ymin)/(ymax - ymin)\n",
        " The equations used to calculate the gradient are the partial derivatives of the cost function with respect to b0 and b1.  \n",
        "\n",
        "$$\\hat y = b_0+b_1x_1$$  \n",
        "$$b_1=\\frac {\\sum\n",
        "(x_n-\\bar x)(y_n-\\bar y)} {\\sum\n",
        "(x_n-\\bar x)^2} $$  \n",
        "$$b_0=\\bar y-b_1\\bar x$$  \n",
        "\n",
        "-  **Step 2**:  \n",
        "Write a function that implements gradient descent from scratch (i.e. you may not call any library functions) to generate the linear regression equation coefficients by minimizing the cost function.\n",
        "Here we use the Sum of Squared Errors (SSE) cost function where n is the number of samples in the dataset, y is the dependent variable in the dataset, $b_0$ is the y-intercept for the linear equation, $b_1$ is the slope of the linear equation, and $x_1$ is the independent variable.\n",
        "\n",
        "$$SSE = \\frac{1}{n}\\sum(y - \\hat y)^2 = \\frac{1}{n}\\sum(y - (b_0+b_1x_1))^2$$  \n",
        "\n",
        "           API:  def gradientDescent(coeffs, X, y, lr, iterations):\n",
        "           Input: coefficients: array of coefficients (i.e. b0 and b1) initialized to random values\n",
        "                  X: normalized training dataset independent vars (see hint below)\n",
        "                  y: normalized training dataset dependent vars (see hint below)\n",
        "                  lr:  learning rate  \n",
        "                  iterations: number of iterations to run\n",
        "           Output: Returns b0_arr, b1_arr, SSE_arr (in this order), where b0_arr, b1_arr, SSE_arr are numpy arrays\n",
        "                   of the b0, b1 coefficients and sum-of-squared error respectively for each of the iterations performed in your gradient descent loop.\n",
        "\n",
        "$$\\frac{\\partial(SSE)}{\\partial(b0)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1))\\cdot -1 = -\\frac{2}{n}(y-\\hat y) $$\n",
        "\n",
        " $$\\frac{\\partial(SSE)}{\\partial(b1)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1))\\cdot -x_1 = -\\frac{2}{n}(y-\\hat y)x_1 $$\n",
        "> Hint:  \n",
        " - The following calculations are done over each sample in the training set (i.e. use a for loop).\n",
        " - Calc the model equation:  yhat = b0 + b1x\n",
        " - Calc the gradient of b0:  sum(-(y-yhat))   \n",
        " - Calc the gradient of b1:  sum(-(y-yhat)*X)\n",
        " - Update the coefficients:  new coef = old coef + (learning rate)(error)(input).  \n",
        "    - Update b0 = b0 - (learning rate) * (gradient of b0)  \n",
        "    - Append calculated b0 to b0 array\n",
        "    - Update b1 = b1 - (learning rate) * (gradient of b1)\n",
        "    - Append calculated b1 to b1 array\n",
        "\n",
        "\n",
        "- **Step 3**:  \n",
        "Plot the equation of your model from the manual (Step 1) model and gradient descent (Step 2) model on the same graph along with your dataset points.  \n",
        "Your plot should look something like this for random initial values of b0 and b1 and 50 iterations (note, it may not look exactly as the plot shown as it will be dependent on the initial random values of the coefficients).  The blue line in the graph is the equation with directly computed coefficients (Step 1), while the red line represents the coefficients calculated by minimizine the cost function with Gradient Descent (Step 2).  Also note that the plotted dataset has been normalized.\n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1F5mxjjzqnuNC9mZNsJwXcHVDcN6TXKgw)\n",
        "\n",
        "\n",
        "- **Step 4**:  \n",
        "Experiment with different numbers of iterations to see how your gradient descent model changes compared to that of manual model.   You should observe an improvement with increased number of iterations such the two techniques converge with higher numbers of iterations. (i.e. the lines should coincide). Print out the the minimum number of iterations, values of your coefficients, and SSE for the line that visually is closest to the direct coefficients method (you should be able to find a number of iterations that basically overlays the two prediction lines). Note that we want to find a minimum number of iterations that also provides a minimized cost.\n",
        "\n",
        "\n",
        "- **Step 5**:  \n",
        "Plot on a 3D graph your coefficients (b0 and b1), on x and y axes respectively, and SSE (Sum of Square Errors) on the z-axis.  Your plot should should the SSE converging to a minimum as shown in the figure below.  Note your graph will not look exactly as the plot shown as it will be depending on the initial random values of the coefficients and the number of iterations.  \n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1buL6OC40DMZJduCQIh66PDBOxYbRieN3)\n",
        "\n",
        "\n",
        "- **Step 6**:  \n",
        "Calculate the performance for both techniques (Step 2, Step 3) using the r-squared score metric.  \n",
        "Hint: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html  \n",
        "Why does the R-square score for the direct calculated coefficients remain constant, and why is it relatively low?  \n",
        "Why does the R-square score change from run to run for the gradient descent calculated coefficients?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "punMFTXRZJBi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import random\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "#Input Parameters\n",
        "Age = np.array([43,21,25,42,57,59,35,15,55,50,65,10,45,35])\n",
        "Glucose = np.array([99,65,79,75,87,81,80,80,90,70,95,67,90,82])\n",
        "\n",
        "\n",
        "#Step 1:  Create a function which MANALLY (using the equations) calculates the\n",
        "#         coefficients for a simple linear regression model, then plot the calculated\n",
        "#         regression line on top of the dataset.\n",
        "#         Your function must input numpy arrays for the x and y variables and return b0 and b1.\n",
        "#         Your function MUST use equations shown in the text cell above.\n",
        "#         Note - you may not use a machine learning library model for this step.\n",
        "#         Note - you may (should) use the function you created in the Linear Prediction Project.\n",
        "#         Note - normalize your data using the min/max method before calculating the coefficients.\n",
        "\n",
        "#Normalize the data using the following min/max method:\n",
        "#Normalized Sample = (Sample - Sample.min)/(Sample.max - Sample.min)\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "plt.scatter(Age,Glucose)\n",
        "\n",
        "#Write a function to calculate the Linear Regression coefficients manually.\n",
        "#Use the function in your previous project on simple linear regression.\n",
        "def simpleLRcoeffsManual(X, Y):\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Print out your coefficients, prediction (yhat or ypred), and the SSE\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "#Plot the data and prediction line for the manual calculation\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Step 2:  Write a function that implements gradient descent to minimize the cost function.\n",
        "#         where the cost function is sum-of-square error (SSE) = 1/n*sum(y - yhat)^2 = 1/n*sum(y-(b0+b1*X))^2\n",
        "#         Inputs:  coeffs:  array of coefficients b0 and b1 initialized to random values\n",
        "#                  X:  dataset normalized independent variable values\n",
        "#                  y:  dataset normalized dependent variable values\n",
        "#                  lr: learning rate\n",
        "#                  iterations:  number of iterations\n",
        "#         Outputs: Returns b0_arr, b1_arr, SSE_arr (in this order)\n",
        "#         def gradientDescent(coeffs, X, y, lr, iterations):\n",
        "#\n",
        "def gradientDescent(coeffs, X, y, lr, iterations):\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "# Step 3: Plot the equation of your model from the manual (Step 1) model and gradient descent (Step 2) model\n",
        "#         on the same graph along with your dataset points.\n",
        "#         Note, Use the normalized dataset.\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "# Step 4:  Experiment with different numbers of iterations to see how your gradient descent\n",
        "#          model changes compared to that of manual model.   You should observe an improvement\n",
        "#          with increased number of iterations such the two techniques converge with higher numbers\n",
        "#          of iterations. (i.e. the lines should coincide). Print out the the minimum number of\n",
        "#          iterations, values of your coefficients, and SSE for the line that visually is closest to the\n",
        "#          direct coefficients method.\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "# Step 5: Plot on a 3D graph your coefficients (b0 and b1), on x and y axes respectively,\n",
        "#         and SSE (Sum of Square Errors) on the z-axis.  Your plot should should the\n",
        "#         SSE converging to a minimum as shown in the figure below.  Note your graph will\n",
        "#         not look exactly as the plot shown as it will be depending on the initial\n",
        "#         random values of the coefficients and the number of iterations.\n",
        "#         Hint: https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "\n",
        "#Step 6:  Calculate the performance for both techniques (Step 2, Step 3) using the r-squared score metric.\n",
        "# YOUR CODE HERE...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import random\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "#Input Parameters\n",
        "Age = np.array([43,21,25,42,57,59,35,15,55,50,65,10,45,35])\n",
        "Glucose = np.array([99,65,79,75,87,81,80,80,90,70,95,67,90,82])\n",
        "\n",
        "plt.scatter(Age,Glucose)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "O30X8yxmpnqh",
        "outputId": "6b7b8f72-f596-4a5c-ec30-97ba79cd94b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fe7a0a7bfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uifTn8ER7vAu"
      },
      "source": [
        "#**PART 2**:  Gradient Descent with Multiple Linear Regression\n",
        "In Part 2, you will train a model as in Part 1, but this time with multiple independent variables.  You will create a generic function to train your model using gradient descent to minimize the cost for varying number of independent variables and a single dependent variable.  You will use the dataset from the previous unit on Multiple Linear regression to train your models.\n",
        "\n",
        "Please reference the video lectures on Gradient Descent, and Multiple Linear Regression for a description the main functions you will need in the implementation.\n",
        "Please complete the following steps in your Colab Script below.  The reference script below provides template code and hints to help with each step.  You will be turning in code and screenshots of your console output in an accompanying assignment.  \n",
        "\n",
        "Recall from Part 1, we used the SSE (sum of squared errors) as our cost function then computed the partial derivatives with respect to each of the coefficients.  We will do the same here, but in a more generic form to support 'n' independent variables.  Recall the cost function we used in Part 1 was the Sum of Squared Errors (SSE) for the simple linear regression model.  Here we expand the cost function for multiple independent variables:\n",
        "\n",
        "$$SSE = \\frac{1}{n}\\sum(y - \\hat y)^2 = \\frac{1}{n}\\sum(y - (b_0+b_1x_1+b_2x_2+...+b_nx_m))^2$$  \n",
        "The gradients of each coefficient are defined as the partial derivatives of the cost function with respect to each of the coefficients.  \n",
        "\n",
        " $$\\frac{\\partial(SSE)}{\\partial(b_0)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1+...+b_nx_n))\\cdot -1 = -(y-\\hat y) $$\n",
        "\n",
        " $$\\frac{\\partial(SSE)}{\\partial(b_1)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1+...+b_nx_n))\\cdot -x_1 = -(y-\\hat y)x_1 $$\n",
        "\n",
        " $$\\frac{\\partial(SSE)}{\\partial(b_2)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1+...+b_nx_n))\\cdot -x_2 = -(y-\\hat y)x_2 $$\n",
        "            <center>**. . .** </center>\n",
        "            \n",
        " $$\\frac{\\partial(SSE)}{\\partial(b_n)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1+...+b_nx_n))\\cdot -x_n = -(y-\\hat y)x_m $$\n",
        "\n",
        "Note that the $b_0$ coefficient is computed slightly different from the remaining coefficients in that all but the $b_0$ coefficient contains its corresponding independent variable term.\n",
        "\n",
        "\n",
        "**Step 1**:   \n",
        "Read in the dataset into Pandas dataframe.  Create a numpy array X by extracting the appropriate column data for the following cases:\n",
        "-  Case 1:  Interest_Rate, Stock_Index_Price\n",
        "-  Case 2:  Interest_Rate, GDP, Stock_Index_Price\n",
        "-  Case 3:  Interest_Rate, Unemployment_Rate, Stock_Index_Price  \n",
        "Note, we include the dependent variable 'Stock_Index_Price' in the X as we'll use it in the training function.  \n",
        "Create a numpy array y with the independent variable: Stock_Index_Price.  \n",
        "\n",
        "**Step 2**:  \n",
        "Write a function to normalize the numpy arrays created in Step 1.  \n",
        "      Normalize cases 1,2,3 for your X and y numpy arrays.  \n",
        ">>      def normalize_dataset(dataset):  \n",
        "\n",
        "**Step 3**:  \n",
        "Write a function that implements gradient descent to minimize the cost function, where the cost function is sum-of-square error as defined above.  \n",
        ">>     def gradientDescent(coeffs, X, lr, iterations):  \n",
        ">>     Inputs:  coeffs:  array of coefficients b0 and b1 initialized to random values   \n",
        "              X:  dataset normalized independent variable values where:  \n",
        "                  x1 is formatted in the first column of the X numpy array  \n",
        "                  x2 is formatted in the second column of the X numpy array  \n",
        "                  xn is formatted in the nth column of the X numpy array  \n",
        "                  y is formmatted in the last column of the X numpy array  \n",
        "              y:  dataset normalized dependent variable values  \n",
        "              Note, that y is contained in the last column of the X array  \n",
        "                    AND separately in the numpy array from Step 2  \n",
        "              lr: learning rate  \n",
        "              iterations:  number of iterations  \n",
        ">>     Outputs: Returns coeff, SSE_arr (in this order)  \n",
        "             Note, the SSE_arr is not required but you should at least return the last SSE.  \n",
        "\n",
        "**Step 4**:  \n",
        "Create (or reuse from the previous linear regression projects) a\n",
        "         function to perform multiple linear regression using sklearn.  \n",
        "         Run cases 1,2,3 with the Gradient descent based regression\n",
        "         and with the sklearn based multiple linear regression.  \n",
        "         Compare the results from each of your test cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky6HpiqY-K6K"
      },
      "source": [
        "from pandas import DataFrame\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "Stock_Market = {'Year':[2018,2018,2018,2018,2018,2018,2018,2018,2018,2018,2018,2018,\\\n",
        "                         2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,\\\n",
        "                         2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\\\n",
        "                'Month':[12,11,10,9,8,7,6,5,4,3,2,1, 12,11,10,9,8,7,6,5,4,3,2,1, 12,11,10,9,8,7,6,5,4,3,2,1],\\\n",
        "                'Interest_Rate':[2.73, 2.7,  2.63, 2.4,  2.37, 2.77, 2.53, 2.23, 2.67, 2.6,  2.43, 2.2,  2.5,  2.47,\\\n",
        "                                 2.03, 2.33, 2.3,  2.57, 2.27, 2.1,  2.,   2.13, 2.07, 2.17, 1.87, 1.73, 1.67, 1.97,\\\n",
        "                                 1.93, 1.9,  1.83, 1.8,  1.77, 1.6,  1.7,  1.63],\\\n",
        "                'Unemployment_Rate':[5.8,  5.75, 5.68, 5.65, 5.63, 5.62, 5.72, 5.83, 5.78, 5.73, 5.7,  5.6,  6.08, 5.67,\\\n",
        "                                     5.97, 5.95, 5.88, 5.87, 5.85, 5.82, 5.77, 5.98, 5.93, 5.92, 5.9,  6.07, 6.05, 6.,\\\n",
        "                                     6.03, 6.02, 6.15, 6.18, 6.17, 6.12, 6.13, 6.1],\\\n",
        "                'GDP':[22.2, 20.6, 21.7, 19.7, 22.1, 19.5, 18.7, 22.,  21.6, 19.4, 21.9, 21.8,\\\n",
        "                       21.5, 21.2, 21.1, 21.,  20.5, 21.4, 20.9, 20.4, 20.,  20.8, 20.3, 20.1,\\\n",
        "                       19.8, 19.6, 20.2, 19.3, 20.7, 18.9, 21.3, 19.2, 19.,  18.8, 19.9, 19.1],\\\n",
        "                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1230,1195,1175,1167,1159,\\\n",
        "                                      1147,1130,1075,1071,1065,1058,1051,1049,1043, 984, 976, 971,\\\n",
        "                                       968, 965, 958, 949, 943, 922, 884, 876, 866, 822, 719, 704]\n",
        "                }\n",
        "\n",
        "\n",
        "#Step 1:  Read in the dataset into Pandas dataframe\n",
        "#         Create a numpy array X by extracting the appropriate column data for the following cases:\n",
        "#         Case 1:  Interest_Rate, Stock_Index_Price\n",
        "#         Case 2:  Interest_Rate, GDP, Stock_Index_Price\n",
        "#         Case 3:  Interest_Rate, Unemployment_Rate, Stock_Index_Price\n",
        "#         Note, we include the dependent variable 'Stock_Index_Price' in the X as we'll use it in the training function.\n",
        "#         Create a numpy array y with the independent variable: Stock_Index_Price\n",
        "#         Hint: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\n",
        "#         Hint: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html\n",
        "#YOUR CODE HERE\n",
        "\n",
        "pd.DataFrame.from_dict(Stock_Market)\n",
        "\n",
        "pd.DataFrame({\"Interest_Rate\": [1, 2], \"Stock_Index_Price\": [3, 4]}).to_numpy()\n",
        "\n",
        "#Step 2:  Write a function to normalize the numpy arrays created in Step 1.\n",
        "#         Normalize cases 1,2,3 for your X and y numpy arrays.\n",
        "#         def normalize_dataset(dataset):\n",
        "def normalize_dataset(dataset):\n",
        "#YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Step 3:  Write a function that implements gradient descent to minimize the cost function.\n",
        "#         where the cost function is sum-of-square error:\n",
        "#         SSE = 1/n*sum(y - yhat)^2 = 1/n*sum(y-(b0+b1*X))^2\n",
        "#         where X is comprised of x1 (indep var 1), x2 (indep var 2), ... xn (indep var n)\n",
        "#         Inputs:  coeffs:  array of coefficients b0 and b1 initialized to random values\n",
        "#                  X:  dataset normalized independent variable values where:\n",
        "#                      x1 is formatted in the first column of the X numpy array\n",
        "#                      x2 is formatted in the second column of the X numpy array\n",
        "#                      xn is formatted in the nth column of the X numpy array\n",
        "#                      y is formmatted in the last column of the X numpy array\n",
        "#                  y:  dataset normalized dependent variable values\n",
        "#                      Note, that y is contained in the last column of the X array\n",
        "#                      AND separately in the numpy array from Step 2\n",
        "#                  lr: learning rate\n",
        "#                  iterations:  number of iterations\n",
        "#         Outputs: Returns coeff, SSE_arr (in this order)\n",
        "#                  Note, the SSE_arr is not required but you should at least return the last SSE\n",
        "#         def gradientDescent(coeffs, X, lr, iterations):\n",
        "\n",
        "def gradientDescent_train(coeffs, X, lr, iterations):\n",
        "# YOUR CODE HERE... (see the inline hints)\n",
        "# Create a numpy array to store the gradients for each of the coeffs (b0, b1, ... bn)\n",
        "# where size = number of indep vars (columns) in the X data + 1\n",
        "# Note, we include num of indep vars + 1 to include b0 = num of cols in X since y is in the dataset\n",
        "\n",
        "# Create an empty numpy array to store the SSE\n",
        "\n",
        "# Loop over 'iterations'\n",
        "#   Init yhat = b0\n",
        "#   Loop over number of indep vars in X\n",
        "#   Note, need to adjust the loop counter to exclude the last col since it will contain the y data\n",
        "#     Update yhat = b0 + b1*x1 + b2x2 + ... bnxn\n",
        "#     Do this for all rows in x1, x2, etc.\n",
        "#     Hint:  use numpy array slicing for x to include all rows\n",
        "#   End Loop over number of indep vars in X\n",
        "\n",
        "#   Compute SSE per the equation in the notes\n",
        "#   Note, make the 1/m term generic, based on the number of samples in X\n",
        "#   Hint: use numpy 'sum' to sum all the squared error terms\n",
        "\n",
        "#   Compute the gradient of b0 per the equation in the notes\n",
        "#   Note, we cannot do this in a loop since it excludes any indep variable\n",
        "\n",
        "#   Loop over number of coefficients - 1 (i.e. b1, b2, ..., bn)\n",
        "#   Note, need to adjust the loop counter with -1 since already computed b0\n",
        "#     Compute the gradient of b1, b2, ... bn per the equation in the notes\n",
        "\n",
        "#   Update the coefficients where each coeff = old coeff - lr*(grad for that coeff)\n",
        "#   Hint: Loop over number of indep vars in X\n",
        "#     SSE_arr = np.append(SSE_arr, SSE)\n",
        "# Return coeffs, SSE_arr\n",
        "#\n",
        "\n",
        "#YOUR CODE HERE...\n",
        "# Test cases - see reference code below\n",
        "'''DIM = 500\n",
        "CASE = 3\n",
        "X_case1 = df[['Interest_Rate','Stock_Index_Price']].to_numpy()\n",
        "X_case2 = df[['Interest_Rate','GDP','Stock_Index_Price']].to_numpy()\n",
        "X_case3 = df[['Interest_Rate','Unemployment_Rate','Stock_Index_Price']].to_numpy()\n",
        "normalize_dataset(X_case1)\n",
        "normalize_dataset(X_case2)\n",
        "normalize_dataset(X_case3)\n",
        "Y_case1 = X_case1[:,-1]\n",
        "Y_case2 = X_case2[:,-1]\n",
        "Y_case3 = X_case3[:,-1]\n",
        "initial_b = np.array([random.random(),random.random(),random.random()])\n",
        "\n",
        "if CASE == 1:\n",
        "  coef, SSE = gradientDescent_train(initial_b, X_case1, 0.01, DIM)\n",
        "  ypred = coef[0] + coef[1]*X_case1[:,0]\n",
        "  print('Score gradient descent calculated coeffs:', r2_score(Y_case1, ypred ))\n",
        "elif CASE ==2:\n",
        "  coef, SSE = gradientDescent_train(initial_b, X_case2, 0.01, DIM)\n",
        "  ypred = coef[0] + coef[1]*X_case2[:,0] + coef[2]*X_case2[:,1]\n",
        "  print('Score gradient descent calculated coeffs:', r2_score(Y_case2, ypred ))\n",
        "else:\n",
        "  coef, SSE = gradientDescent_train(initial_b, X_case3, 0.01, DIM)\n",
        "  ypred = coef[0] + coef[1]*X_case3[:,0] + coef[2]*X_case3[:,1]\n",
        "  print('Score gradient descent calculated coeffs:', r2_score(Y_case3, ypred ))\n",
        "\n",
        "print('Start, Stop SSE:', SSE[0], SSE[DIM-1])\n",
        "'''\n",
        "\n",
        "\n",
        "#Step 4:  Create (or reuse from the previous linear regression projects) a\n",
        "#         function to perform multiple linear regression using sklearn.\n",
        "#         Run cases 1,2,3 with the Gradient descent based regression\n",
        "#         and with the sklearn based multiple linear regression.\n",
        "#         Compare the results from each.\n",
        "#YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "Stock_Market = {'Year':[2018,2018,2018,2018,2018,2018,2018,2018,2018,2018,2018,2018,\\\n",
        "                         2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,\\\n",
        "                         2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\\\n",
        "                'Month':[12,11,10,9,8,7,6,5,4,3,2,1, 12,11,10,9,8,7,6,5,4,3,2,1, 12,11,10,9,8,7,6,5,4,3,2,1],\\\n",
        "                'Interest_Rate':[2.73, 2.7,  2.63, 2.4,  2.37, 2.77, 2.53, 2.23, 2.67, 2.6,  2.43, 2.2,  2.5,  2.47,\\\n",
        "                                 2.03, 2.33, 2.3,  2.57, 2.27, 2.1,  2.,   2.13, 2.07, 2.17, 1.87, 1.73, 1.67, 1.97,\\\n",
        "                                 1.93, 1.9,  1.83, 1.8,  1.77, 1.6,  1.7,  1.63],\\\n",
        "                'Unemployment_Rate':[5.8,  5.75, 5.68, 5.65, 5.63, 5.62, 5.72, 5.83, 5.78, 5.73, 5.7,  5.6,  6.08, 5.67,\\\n",
        "                                     5.97, 5.95, 5.88, 5.87, 5.85, 5.82, 5.77, 5.98, 5.93, 5.92, 5.9,  6.07, 6.05, 6.,\\\n",
        "                                     6.03, 6.02, 6.15, 6.18, 6.17, 6.12, 6.13, 6.1],\\\n",
        "                'GDP':[22.2, 20.6, 21.7, 19.7, 22.1, 19.5, 18.7, 22.,  21.6, 19.4, 21.9, 21.8,\\\n",
        "                       21.5, 21.2, 21.1, 21.,  20.5, 21.4, 20.9, 20.4, 20.,  20.8, 20.3, 20.1,\\\n",
        "                       19.8, 19.6, 20.2, 19.3, 20.7, 18.9, 21.3, 19.2, 19.,  18.8, 19.9, 19.1],\\\n",
        "                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1230,1195,1175,1167,1159,\\\n",
        "                                      1147,1130,1075,1071,1065,1058,1051,1049,1043, 984, 976, 971,\\\n",
        "                                       968, 965, 958, 949, 943, 922, 884, 876, 866, 822, 719, 704]\n",
        "                }\n",
        "\n",
        "\n",
        "pd.DataFrame.from_dict(Stock_Market)\n",
        "\n",
        "pd.DataFrame({\"Interest_Rate\": [1, 2], \"Stock_Index_Price\": [3, 4]}).to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtAxuR_dq3FG",
        "outputId": "4049301e-ee22-4db4-ba87-604c03cdd6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 3],\n",
              "       [2, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}